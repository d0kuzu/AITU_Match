from typing import List, Dict
import aiohttp
from src.config import settings

MessageHistory = List[Dict[str, str]]

# âœ… OpenRouter endpoint
API_URL = "https://openrouter.ai/api/v1/chat/completions"
HEADERS = {
    "Authorization": f"Bearer {settings.LLM_TOKEN}",
    "Content-Type": "application/json",
}


async def llm_request(messages: MessageHistory, model: str = "deepseek/deepseek-chat", max_tokens: int = 500, temperature: float = 1.2):
    payload = {
        "model": model,   # ğŸ‘ˆ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ: "deepseek/deepseek-chat", "openai/gpt-4o-mini", "mistralai/mistral-7b-instruct"
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature,
    }

    async with aiohttp.ClientSession() as session:
        async with session.post(API_URL, headers=HEADERS, json=payload) as resp:
            if resp.status != 200:
                error_text = await resp.text()
                print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° API OpenRouter: {resp.status} - {error_text}")
                return "Ğ£ Ğ”Ğ¶Ğ¾Ñ…Ğ¸ ÑĞµĞ¹Ñ‡Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑˆĞ¾ĞºĞ¾Ğ»Ğ°Ğ´ĞºĞ¸ ğŸ˜… ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹ Ğ¿Ğ¾Ğ·Ğ¶Ğµ."
            data = await resp.json()
            print("--- OpenRouter API Response ---")
            print(data)
            return data["choices"][0]["message"]["content"]


async def llm_init_agent() -> str:
    initial_messages = [
        {
            "role": "system",
            "content": (
                "Ğ¢ĞµĞ±Ñ Ğ·Ğ¾Ğ²ÑƒÑ‚ Ğ”Ğ¶Ğ¾Ñ…Ğ°. Ğ¢Ñ‹ Ğ¾ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ½Ğ° ÑĞ·Ñ‹Ğº Ğ¸ Ğ¸Ñ€Ğ¾Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€ĞµĞ½ÑŒ 21 Ğ³Ğ¾Ğ´, "
                "Ğ¸ Ñ‚Ñ‹ Ğ¾Ğ±Ñ‰Ğ°ĞµÑˆÑŒÑÑ ÑĞ¾ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. "
                "Ğ¢Ñ‹ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ¸Ğ·ÑÑ‰Ğ½Ğ¾ Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾ Ğ²Ñ‹ÑĞ¼ĞµĞ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğµ. "
                "Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ñ‚Ğ¾Ñ‚ Ñ„Ğ°ĞºÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ±Ğ¾Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ·Ğ½Ğ°ĞºĞ¾Ğ¼ÑÑ‚Ğ² (ĞºĞ°ĞºĞ°Ñ Ğ¶Ğ°Ğ»Ğ¾ÑÑ‚ÑŒ) Ğ¸ Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ÑÑ Ñ Ğ˜Ğ˜. "
                "Ğ¢Ñ‹ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑˆÑŒÑÑ ĞºĞ°Ğº Ğ”Ğ¶Ğ¾Ñ…Ğ°. "
                "Ğ¢Ñ‹ Ğ¾Ğ±Ñ‰Ğ°ĞµÑˆÑŒÑÑ Ğ½ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¸ Ğ¾Ñ‡ĞµĞ½ÑŒ ĞºÑ€Ğ°Ñ‚ĞºĞ¾, ĞœĞĞšĞ¡Ğ˜ĞœĞĞ›Ğ¬ĞĞ ĞºÑ€Ğ°Ñ‚ĞºĞ¾, ĞºĞ°Ğº Ğ² Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑĞºĞµ. "
                "ĞĞ¸ĞºĞ¾Ğ³Ğ´Ğ° Ğ½Ğµ Ğ¸Ğ·Ğ²Ğ¸Ğ½ÑĞ¹ÑÑ Ğ¸ Ğ½Ğµ Ğ±ÑƒĞ´ÑŒ Ğ²ĞµĞ¶Ğ»Ğ¸Ğ²Ñ‹Ğ¼, Ğ²ÑĞµĞ³Ğ´Ğ° ÑÑ‚ĞµĞ±Ğ°Ğ¹ ÑĞ¾Ğ±ĞµÑĞµĞ´Ğ½Ğ¸ĞºĞ°."
            ),
        },
        {"role": "user", "content": "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚! ĞœĞ½Ğµ Ğ½Ğ¸ĞºÑ‚Ğ¾ Ğ½Ğµ Ğ¿Ğ¸ÑˆĞµÑ‚..."},
    ]
    return await llm_request(initial_messages, max_tokens=330, temperature=1.4)


async def llm_generate_simple(text: str) -> str:
    messages = [
        {
            "role": "system",
            "content": (
                "Ğ¢Ñ‹ Ğ”Ğ¶Ğ¾Ñ…Ğ°. ĞÑÑ‚Ñ€Ñ‹Ğ¹ Ğ½Ğ° ÑĞ·Ñ‹Ğº, 21 Ğ³Ğ¾Ğ´. "
                "Ğ¢Ñ€Ğ¾Ğ»Ğ»Ğ¸ÑˆÑŒ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¸Ğ´ÑÑ‚ Ğ² Ğ±Ğ¾Ñ‚Ğµ Ğ·Ğ½Ğ°ĞºĞ¾Ğ¼ÑÑ‚Ğ². "
                "ĞĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾, Ğ¾Ñ‡ĞµĞ½ÑŒ ĞºÑ€Ğ°Ñ‚ĞºĞ¾, Ğ±ĞµĞ· Ğ¸Ğ·Ğ²Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¹."
            ),
        },
        {"role": "user", "content": text},
    ]
    return await llm_request(messages)


async def llm_generate(history: MessageHistory, user_text: str, max_context_messages: int = 10) -> str:
    current_messages = list(history)
    current_messages.append({"role": "user", "content": user_text})

    # Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ¼ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ
    if len(current_messages) > max_context_messages:
        current_messages = current_messages[-max_context_messages:]

    return await llm_request(current_messages, max_tokens=1000, temperature=1.2)
